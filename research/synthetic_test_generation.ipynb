{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac9cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Dict, List\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd4171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PG_DSN = (\n",
    "    f\"postgresql://{os.getenv('PG_USER')}:\"\n",
    "    f\"{os.getenv('PG_PASS')}@\"\n",
    "    f\"{os.getenv('PG_HOST')}:\"\n",
    "    f\"{os.getenv('PG_PORT')}/\"\n",
    "    f\"{os.getenv('PG_DB')}\"\n",
    ")\n",
    "OPENROUTER_KEY = os.getenv('OPENROUTER_KEY')\n",
    "engine = create_engine(PG_DSN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbb9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fetch_random_news\n",
    "\n",
    "class GPTSynthesizer:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "    def _gpt_query(self, system_prompt: str, prompt: str) -> str:\n",
    "        messages: List[Dict[str, str]] = []\n",
    "        if system_prompt:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Всегда отвечай на русском языке. \" + system_prompt\n",
    "            })\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"qwen/qwen-2.5-72b-instruct\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    def generate_for_article(self, title: str, anons: str, body: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Для переданной статьи (title, anons, body) формирует 3 поисковых запроса\n",
    "        с разными уровнями сложности: лёгкий, средний, сложный.\n",
    "        Возвращает словарь {\"easy\": ..., \"medium\": ..., \"hard\": ...}.\n",
    "        \"\"\"\n",
    "        # Собираем контекст: комбинируем title + anons (если есть) + первые 200 символов body\n",
    "        snippet = body[:200].replace(\"\\n\", \" \")\n",
    "        full_context = f\"Заголовок: {title}\\nАнонс: {anons or ''}\\nТело (фрагмент): {snippet}...\"\n",
    "        system_prompt = (\n",
    "            \"Ты генерируешь три поисковых запроса разного уровня сложности, \"\n",
    "            \"чтобы найти именно эту новость на новостном портале. Важно, чтобы запрос был именно к этой новости\"\n",
    "            \"Первый запрос — максимально буквальный, в точности по ключевым словам. \"\n",
    "            \"Второй — чуть более обобщённый (с синонимами). \"\n",
    "            \"Третий — максимально абстрактный, где используются контекстные формулировки и косвенные признаки, возможны реалестичные опечатки, чтобы тестировать поисковую систему по новостям\"\n",
    "        )\n",
    "        prompt = (\n",
    "            f\"{full_context}\\n\\n\"\n",
    "            \"Сгенерируй три отдельных строки, помеченные как:\\n\"\n",
    "            \"1) Лёгкий запрос: <здесь>\\n\"\n",
    "            \"2) Средний запрос: <здесь>\\n\"\n",
    "            \"3) Сложный запрос: <здесь>\"\n",
    "        )\n",
    "        response = self._gpt_query(system_prompt, prompt)\n",
    "        lines = [line.strip() for line in response.split(\"\\n\") if line.strip()]\n",
    "        result = {\"easy\": \"\", \"medium\": \"\", \"hard\": \"\"}\n",
    "        for line in lines:\n",
    "            if line.startswith(\"1\") or line.lower().startswith(\"1)\"):\n",
    "                result[\"easy\"] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"2\") or line.lower().startswith(\"2)\"):\n",
    "                result[\"medium\"] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"3\") or line.lower().startswith(\"3)\"):\n",
    "                result[\"hard\"] = line.split(\":\", 1)[1].strip()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = fetch_random_news(limit=100, sample_pct=1.0)\n",
    "gpt = GPTSynthesizer(api_key=OPENROUTER_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52545061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:01<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "async def _process_row(executor, row):\n",
    "    art_id = row[\"id\"]\n",
    "    title, anons, body = row[\"title\"], row[\"anons\"], row[\"body\"]\n",
    "\n",
    "    gens = await asyncio.get_running_loop().run_in_executor(\n",
    "        executor, lambda: gpt.generate_for_article(title, anons, body)\n",
    "    )\n",
    "    return art_id, gens\n",
    "\n",
    "async def generate_synthetic(df):\n",
    "    synthetic = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        tasks = [\n",
    "            asyncio.create_task(_process_row(executor, row))\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "            art_id, gens = await coro\n",
    "            synthetic.append((art_id, \"easy\",   gens[\"easy\"]))\n",
    "            synthetic.append((art_id, \"medium\", gens[\"medium\"]))\n",
    "            synthetic.append((art_id, \"hard\",   gens[\"hard\"]))\n",
    "    return synthetic\n",
    "\n",
    "\n",
    "synthetic_queries = await generate_synthetic(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "076f406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 1459734.57it/s]\n"
     ]
    }
   ],
   "source": [
    "df_synth = pd.DataFrame(synthetic_queries, columns=[\"article_id\", \"difficulty\", \"query\"])\n",
    "queries_gt: Dict[str, List[str]] = {}\n",
    "for art_id, difficulty, q in tqdm(synthetic_queries):\n",
    "    queries_gt[q] = [art_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de85308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synth.to_csv(\"synthetic_queries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1764372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
