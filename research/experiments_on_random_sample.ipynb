{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970dfc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from processing.tfidf import TfidfResearch\n",
    "from processing.bm25 import BM25Research\n",
    "from processing.faiss_index import FaissResearch\n",
    "from processing.qdrant_index import QdrantResearch\n",
    "\n",
    "from sqlalchemy import text\n",
    "from processing.utils import engine\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from processing.eval import run_and_log\n",
    "import numpy as np\n",
    "from processing.utils import clean_text, STOP_RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179d805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries_gt(df: pd.DataFrame) -> tuple[dict[str, list[str]], list[str]]:    \n",
    "    q_gt: dict[str, list[str]] = {}\n",
    "    for _, row in df.iterrows():\n",
    "        q = row[\"query\"]\n",
    "        art_id = str(row[\"article_id\"])\n",
    "        q_gt.setdefault(q, []).append(art_id)\n",
    "\n",
    "    all_ids = df[\"article_id\"].astype(str).unique().tolist()\n",
    "    return q_gt, all_ids\n",
    "\n",
    "def queries_by_diff(df: pd.DataFrame) -> dict[str, dict[str, list[str]]]:\n",
    "    groups: dict[str, dict[str, list[str]]] = {}\n",
    "    for diff in (\"easy\", \"medium\", \"hard\"):\n",
    "        sub = df[df[\"difficulty\"] == diff]\n",
    "        qgt = {q: [str(doc)] for q, doc in zip(sub[\"query\"], sub[\"article_id\"])}\n",
    "        groups[diff] = qgt\n",
    "\n",
    "    groups[\"all\"] = {q: [str(doc)] for q, doc in zip(df[\"query\"], df[\"article_id\"])}\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162c8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_random = \"synthetic_data/synthetic_queries_random_10000.csv\"\n",
    "df_synth_random = pd.read_csv(csv_path_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa87b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total requests: 29863, Documents in corpus: 10000\n",
      "Groups keys: ['easy', 'medium', 'hard', 'all']\n"
     ]
    }
   ],
   "source": [
    "queries_gt, article_ids = load_queries_gt(df_synth_random)\n",
    "\n",
    "q_groups = queries_by_diff(df_synth_random)\n",
    "\n",
    "print(f\"Total requests: {len(queries_gt)}, Documents in corpus: {len(article_ids)}\")\n",
    "print(f\"Groups keys: {list(q_groups.keys())}\")  # easy, medium, hard, all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7a5a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News example: Американские самолеты мешали российским бомбить игиловцев. Российские самолеты неоднократно сближались в американскими в районе долины реки Евфрат в С …\n"
     ]
    }
   ],
   "source": [
    "sql = text(\"\"\"\n",
    "    SELECT id::text,\n",
    "           COALESCE(title, '')   AS title,\n",
    "           COALESCE(anons, '')   AS anons,\n",
    "           COALESCE(body, '')    AS body\n",
    "      FROM public.tmp_news\n",
    "     WHERE id = ANY(:ids);\n",
    "\"\"\")\n",
    "\n",
    "df_docs = pd.read_sql(sql, engine, params={\"ids\": article_ids})\n",
    "\n",
    "\n",
    "df_docs[\"full_text\"] = (\n",
    "    df_docs[\"title\"] + \". \" +\n",
    "    df_docs[\"anons\"] + \". \" +\n",
    "    df_docs[\"body\"]\n",
    ")\n",
    "texts = df_docs[\"full_text\"].tolist()\n",
    "ids   = df_docs[\"id\"].tolist()\n",
    "\n",
    "print(\"News example:\", texts[0][:150], \"…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651ffec",
   "metadata": {},
   "source": [
    "Base Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec593bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:00<00:00, 12217.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         test_name backend   level  Precision@10  Recall@10       MRR\n",
      "0  tfidf_30k_vocab   tfidf    easy      0.097006   0.970060  0.966068\n",
      "1  tfidf_30k_vocab   tfidf  medium      0.097006   0.970060  0.929142\n",
      "2  tfidf_30k_vocab   tfidf    hard      0.094578   0.945783  0.892771\n",
      "3  tfidf_30k_vocab   tfidf     all      0.096200   0.962000  0.929400\n",
      "4  tfidf_30k_vocab   tfidf     all      0.096200   0.962000  0.929400\n"
     ]
    }
   ],
   "source": [
    "tfidf_backend = TfidfResearch(max_features=30_000)\n",
    "tfidf_backend.index(tqdm(texts), ids)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = tfidf_backend,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"tfidf\",\n",
    "    test_name    = \"tfidf_30k_vocab\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f3e53f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test_name backend   level  Precision@10  Recall@10       MRR\n",
      "0      bm25    bm25    easy      0.097605   0.976048  0.968064\n",
      "1      bm25    bm25  medium      0.097605   0.976048  0.940868\n",
      "2      bm25    bm25    hard      0.095181   0.951807  0.903614\n",
      "3      bm25    bm25     all      0.096800   0.968000  0.937583\n",
      "4      bm25    bm25     all      0.096800   0.968000  0.937583\n"
     ]
    }
   ],
   "source": [
    "bm25_backend = BM25Research()\n",
    "bm25_backend.index(texts, ids)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = bm25_backend,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"bm25\",\n",
    "    test_name    = \"bm25\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31a66454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:00<00:00, 6711.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                test_name      backend   level  Precision@10  Recall@10  \\\n",
      "0  Faiss + BERTA, dim=768  faiss_berta    easy      0.098802   0.988024   \n",
      "1  Faiss + BERTA, dim=768  faiss_berta  medium      0.098204   0.982036   \n",
      "2  Faiss + BERTA, dim=768  faiss_berta    hard      0.096988   0.969880   \n",
      "3  Faiss + BERTA, dim=768  faiss_berta     all      0.098000   0.980000   \n",
      "4  Faiss + BERTA, dim=768  faiss_berta     all      0.098000   0.980000   \n",
      "\n",
      "        MRR  \n",
      "0  0.958084  \n",
      "1  0.958583  \n",
      "2  0.954819  \n",
      "3  0.957167  \n",
      "4  0.957167  \n"
     ]
    }
   ],
   "source": [
    "faiss_berta = FaissResearch(model_name=\"sergeyzh/BERTA\", embed_dim=768)\n",
    "faiss_berta.index(tqdm(texts), ids)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = faiss_berta,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"faiss_berta\",\n",
    "    test_name    = \"Faiss + BERTA, dim=768\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9602ae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:00<00:00, 11155.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 test_name      backend   level  Precision@10  Recall@10  \\\n",
      "0  Faiss + BERTA, dim=1024  faiss_berta    easy      0.098802   0.988024   \n",
      "1  Faiss + BERTA, dim=1024  faiss_berta  medium      0.098204   0.982036   \n",
      "2  Faiss + BERTA, dim=1024  faiss_berta    hard      0.096988   0.969880   \n",
      "3  Faiss + BERTA, dim=1024  faiss_berta     all      0.098000   0.980000   \n",
      "4  Faiss + BERTA, dim=1024  faiss_berta     all      0.098000   0.980000   \n",
      "\n",
      "        MRR  \n",
      "0  0.958084  \n",
      "1  0.958583  \n",
      "2  0.954819  \n",
      "3  0.957167  \n",
      "4  0.957167  \n"
     ]
    }
   ],
   "source": [
    "faiss_berta = FaissResearch(model_name=\"sergeyzh/BERTA\", embed_dim=1024)\n",
    "faiss_berta.index(tqdm(texts), ids)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = faiss_berta,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"faiss_berta\",\n",
    "    test_name    = \"Faiss + BERTA, dim=1024\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "028d9771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 test_name         backend   level  Precision@10  Recall@10  \\\n",
      "0  QDRANT + BERTA, dim=768  qdrant_backend    easy      0.098802   0.988024   \n",
      "1  QDRANT + BERTA, dim=768  qdrant_backend  medium      0.098204   0.982036   \n",
      "2  QDRANT + BERTA, dim=768  qdrant_backend    hard      0.096988   0.969880   \n",
      "3  QDRANT + BERTA, dim=768  qdrant_backend     all      0.098000   0.980000   \n",
      "4  QDRANT + BERTA, dim=768  qdrant_backend     all      0.098000   0.980000   \n",
      "\n",
      "        MRR  \n",
      "0  0.958084  \n",
      "1  0.958583  \n",
      "2  0.953815  \n",
      "3  0.956833  \n",
      "4  0.956833  \n"
     ]
    }
   ],
   "source": [
    "qdrant_backend = QdrantResearch(\n",
    "    collection_name=\"news_research_synth\",\n",
    "    model_name=\"sergeyzh/BERTA\",\n",
    "    embed_dim=768,\n",
    ")\n",
    "qdrant_backend.index(texts, ids)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = qdrant_backend,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"qdrant_backend\",\n",
    "    test_name    = \"QDRANT + BERTA, dim=768\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de543e6",
   "metadata": {},
   "source": [
    "*Additional experiments*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc3be6",
   "metadata": {},
   "source": [
    "Lemmatise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cc72ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pymorphy lemmatize: 100%|██████████| 167/167 [00:02<00:00, 63.53it/s]\n",
      "lemmatize queries: 100%|██████████| 500/500 [00:00<00:00, 1372.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "if not hasattr(inspect, \"getargspec\"):\n",
    "    inspect.getargspec = inspect.getfullargspec\n",
    "\n",
    "from pymorphy3 import MorphAnalyzer \n",
    "import nltk, re\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def lemmatize_text(txt: str) -> str:\n",
    "    tokens = [tok for tok in nltk.word_tokenize(txt.lower()) if tok.isalpha()]\n",
    "    lemmas = [\n",
    "        morph.parse(tok)[0].normal_form\n",
    "        for tok in tokens\n",
    "        if len(tok) > 2\n",
    "    ]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "\n",
    "texts_lemma = [lemmatize_text(t) for t in tqdm(texts, desc=\"pymorphy lemmatize\")]\n",
    "queries_gt_lemma: dict[str, list[str]] = {}\n",
    "\n",
    "for q, ids_list in tqdm(queries_gt.items(), desc=\"lemmatize queries\"):\n",
    "    lem_q = lemmatize_text(q)  \n",
    "\n",
    "    queries_gt_lemma.setdefault(lem_q, []).extend(ids_list)\n",
    "\n",
    "\n",
    "for k in queries_gt_lemma:\n",
    "    queries_gt_lemma[k] = list(set(queries_gt_lemma[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa8ee159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     test_name      backend   level  Precision@10  Recall@10  \\\n",
      "0  tfidf_30k_vocab + lemmatize  tfidf_lemma    easy      0.083832   0.838323   \n",
      "1  tfidf_30k_vocab + lemmatize  tfidf_lemma  medium      0.084431   0.844311   \n",
      "2  tfidf_30k_vocab + lemmatize  tfidf_lemma    hard      0.085542   0.855422   \n",
      "3  tfidf_30k_vocab + lemmatize  tfidf_lemma     all      0.084600   0.846000   \n",
      "4  tfidf_30k_vocab + lemmatize  tfidf_lemma     all      0.084600   0.846000   \n",
      "\n",
      "        MRR  \n",
      "0  0.705589  \n",
      "1  0.703358  \n",
      "2  0.776520  \n",
      "3  0.728393  \n",
      "4  0.728393  \n"
     ]
    }
   ],
   "source": [
    "tfidf_lemma = TfidfResearch(max_features=30_000)\n",
    "tfidf_lemma.index(texts_lemma, ids)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = tfidf_lemma,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"tfidf_lemma\",\n",
    "    test_name    = \"tfidf_30k_vocab + lemmatize\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bb5510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            test_name            backend   level  \\\n",
      "0  Faiss + BERTA, dim=768 + lemmatize  faiss_berta_lemma    easy   \n",
      "1  Faiss + BERTA, dim=768 + lemmatize  faiss_berta_lemma  medium   \n",
      "2  Faiss + BERTA, dim=768 + lemmatize  faiss_berta_lemma    hard   \n",
      "3  Faiss + BERTA, dim=768 + lemmatize  faiss_berta_lemma     all   \n",
      "4  Faiss + BERTA, dim=768 + lemmatize  faiss_berta_lemma     all   \n",
      "\n",
      "   Precision@10  Recall@10       MRR  \n",
      "0      0.098802   0.988024  0.936078  \n",
      "1      0.097605   0.976048  0.948959  \n",
      "2      0.096988   0.969880  0.950803  \n",
      "3      0.097800   0.978000  0.945269  \n",
      "4      0.097800   0.978000  0.945269  \n"
     ]
    }
   ],
   "source": [
    "faiss_berta_lemma = FaissResearch(model_name=\"sergeyzh/BERTA\", embed_dim=768)\n",
    "faiss_berta_lemma.index(texts_lemma, ids)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = faiss_berta_lemma,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"faiss_berta_lemma\",\n",
    "    test_name    = \"Faiss + BERTA, dim=768 + lemmatize\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe44f2",
   "metadata": {},
   "source": [
    "Spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9604d5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [06:13<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             test_name     backend   level  Precision@10  Recall@10       MRR\n",
      "0  BM25 + spellchecker  bm25_spell    easy      0.097006   0.970060  0.967066\n",
      "1  BM25 + spellchecker  bm25_spell  medium      0.097006   0.970060  0.939870\n",
      "2  BM25 + spellchecker  bm25_spell    hard      0.094578   0.945783  0.902610\n",
      "3  BM25 + spellchecker  bm25_spell     all      0.096200   0.962000  0.936583\n",
      "4  BM25 + spellchecker  bm25_spell     all      0.096200   0.962000  0.936583\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker(language=\"ru\")\n",
    "\n",
    "def spell_correct(txt: str) -> str:\n",
    "    toks  = txt.split()\n",
    "    fixed = [spell.correction(t) or t for t in toks]\n",
    "    return \" \".join(fixed)\n",
    "\n",
    "\n",
    "queries_gt_spell = {spell_correct(q): gts for q, gts in tqdm(queries_gt.items())}\n",
    "\n",
    "bm25_spell = BM25Research()\n",
    "bm25_spell.index(texts, ids)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = bm25_spell,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"bm25_spell\",\n",
    "    test_name    = \"BM25 + spellchecker\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fce1e1",
   "metadata": {},
   "source": [
    "Hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d6d6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_full = FaissResearch(model_name=\"sergeyzh/BERTA\")\n",
    "faiss_full.index(texts, ids)                \n",
    "doc_embs      = faiss_full.embeddings       \n",
    "id2pos: dict  = {doc_id: i for i, doc_id in enumerate(ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11e798c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_fast(query: str, N: int = 100, top_k: int = 10):\n",
    "    top_tfidf = tfidf_lemma.search(query, top_k=N)\n",
    "    sub_idx   = np.array([id2pos[d] for d, _ in top_tfidf], dtype=np.int64)\n",
    "\n",
    "    q_vec = faiss_full._get_embeddings([query])   \n",
    "\n",
    "    scores = np.squeeze(q_vec @ doc_embs[sub_idx].T)\n",
    "\n",
    "    order  = np.argsort(scores)[::-1][:top_k]\n",
    "    result = [(ids[sub_idx[i]], float(scores[i])) for i in order]\n",
    "    return result\n",
    "\n",
    "\n",
    "class HybridFastBackend:\n",
    "    def __init__(self, N=100, top_k=10):\n",
    "        self.N = N\n",
    "        self.top_k = top_k\n",
    "    def index(self, *args, **kwargs):\n",
    "        pass                         \n",
    "    def search(self, q, top_k=10):\n",
    "        return hybrid_search_fast(q, N=self.N, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bc0e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    test_name      backend   level  \\\n",
      "0  Hybrid Faiss + BERTA, dim=768 + BM25, N=50  hybrid_fast    easy   \n",
      "1  Hybrid Faiss + BERTA, dim=768 + BM25, N=50  hybrid_fast  medium   \n",
      "2  Hybrid Faiss + BERTA, dim=768 + BM25, N=50  hybrid_fast    hard   \n",
      "3  Hybrid Faiss + BERTA, dim=768 + BM25, N=50  hybrid_fast     all   \n",
      "4  Hybrid Faiss + BERTA, dim=768 + BM25, N=50  hybrid_fast     all   \n",
      "\n",
      "   Precision@10  Recall@10       MRR  \n",
      "0      0.098802   0.988024  0.958084  \n",
      "1      0.098802   0.988024  0.959182  \n",
      "2      0.096988   0.969880  0.957831  \n",
      "3      0.098200   0.982000  0.958367  \n",
      "4      0.098200   0.982000  0.958367  \n"
     ]
    }
   ],
   "source": [
    "hybrid_fast = HybridFastBackend(N=200)\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = hybrid_fast,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"hybrid_fast\",\n",
    "    test_name    = \"Hybrid Faiss + BERTA, dim=768 + BM25, N=50\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd642f",
   "metadata": {},
   "source": [
    "Named entity recognition improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39214af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER payload: 100%|██████████| 167/167 [00:01<00:00, 101.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsNERTagger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "segmenter = Segmenter()\n",
    "emb       = NewsEmbedding()\n",
    "ner       = NewsNERTagger(emb)\n",
    "\n",
    "def extract_ents(text: str) -> list[str]:\n",
    "    doc = Doc(text[:600])\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner)\n",
    "    return [f\"{s.type}:{s.text.lower()}\" for s in doc.spans]\n",
    "\n",
    "\n",
    "from processing.faiss_index import FaissResearch \n",
    "\n",
    "faiss_full = FaissResearch(model_name=\"sergeyzh/BERTA\")  \n",
    "faiss_full.index(texts, ids)         \n",
    "\n",
    "\n",
    "faiss_full.id2ents = {\n",
    "    str(ids[i]): extract_ents(texts[i])\n",
    "    for i in tqdm(range(len(ids)), desc=\"NER payload\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a736d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_fast(query: str, N: int = 100, top_k: int = 10):\n",
    "    top_tfidf = tfidf_lemma.search(query, top_k=N)\n",
    "    sub_idx   = np.array([id2pos[d] for d, _ in top_tfidf], dtype=np.int64)\n",
    "\n",
    "    q_vec  = faiss_full._get_embeddings([query])        \n",
    "    scores = np.squeeze(q_vec @ doc_embs[sub_idx].T)    \n",
    "\n",
    "    order  = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(ids[sub_idx[i]], float(scores[i])) for i in order]\n",
    "\n",
    "class HybridFastBackend:\n",
    "    def __init__(self, N=100):\n",
    "        self.N = N\n",
    "    def index(self, *args, **kwargs):          \n",
    "        pass\n",
    "    def search(self, q, top_k=10):\n",
    "        return hybrid_search_fast(q, self.N, top_k)\n",
    "\n",
    "\n",
    "def hybrid_search_fast_ner(query: str, N: int = 100, top_k: int = 10,\n",
    "                           alpha: float = 0.2):\n",
    "    top_tfidf = tfidf_lemma.search(query, top_k=N)\n",
    "    sub_idx   = np.array([id2pos[d] for d, _ in top_tfidf], dtype=np.int64)\n",
    "\n",
    "    q_vec  = faiss_full._get_embeddings([query])\n",
    "    scores = np.squeeze(q_vec @ doc_embs[sub_idx].T)\n",
    "\n",
    "    ents_q = set(extract_ents(query))\n",
    "\n",
    "    for j, doc_idx in enumerate(sub_idx):\n",
    "        overlap = len(ents_q & set(faiss_full.id2ents.get(ids[doc_idx], [])))\n",
    "        scores[j] += alpha * overlap\n",
    "\n",
    "    order  = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(ids[sub_idx[i]], float(scores[i])) for i in order]\n",
    "\n",
    "class HybridFastNERBackend(HybridFastBackend):\n",
    "    def __init__(self, N=100, alpha=0.2):\n",
    "        super().__init__(N)\n",
    "        self.alpha = alpha\n",
    "    def search(self, q, top_k=10):\n",
    "        return hybrid_search_fast_ner(q, self.N, top_k, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cef0d31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER payload: 100%|██████████| 167/167 [00:01<00:00, 99.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           test_name     backend   level  \\\n",
      "0  Hybrid Faiss + BERTA, dim=768 + BM25, N=50 + N...  hybrid_ner    easy   \n",
      "1  Hybrid Faiss + BERTA, dim=768 + BM25, N=50 + N...  hybrid_ner  medium   \n",
      "2  Hybrid Faiss + BERTA, dim=768 + BM25, N=50 + N...  hybrid_ner    hard   \n",
      "3  Hybrid Faiss + BERTA, dim=768 + BM25, N=50 + N...  hybrid_ner     all   \n",
      "4  Hybrid Faiss + BERTA, dim=768 + BM25, N=50 + N...  hybrid_ner     all   \n",
      "\n",
      "   Precision@10  Recall@10       MRR  \n",
      "0      0.099401   0.994012  0.963181  \n",
      "1      0.098204   0.982036  0.966816  \n",
      "2      0.096386   0.963855  0.933735  \n",
      "3      0.098000   0.980000  0.954619  \n",
      "4      0.098000   0.980000  0.954619  \n"
     ]
    }
   ],
   "source": [
    "hybrid_ner = HybridFastNERBackend(N=200, alpha=0.3)\n",
    "faiss_full.id2ents = {\n",
    "    str(ids[i]): extract_ents(texts[i])\n",
    "    for i in tqdm(range(len(ids)), desc=\"NER payload\")\n",
    "}\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = hybrid_ner,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"hybrid_ner\",\n",
    "    test_name    = \"Hybrid Faiss + BERTA, dim=768 + BM25, N=50 + NER with a=0.3\",\n",
    "    top_k        = 10,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421b7da",
   "metadata": {},
   "source": [
    "Hybrid search with augmented query (like in prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe99b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:00<00:00, 1476.75it/s]\n",
      "                                                         \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>backend</th>\n",
       "      <th>level</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hybrid BM25→FAISS + GPT-aug</td>\n",
       "      <td>hybrid_gpt_aug</td>\n",
       "      <td>easy</td>\n",
       "      <td>0.099401</td>\n",
       "      <td>0.994012</td>\n",
       "      <td>0.994012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hybrid BM25→FAISS + GPT-aug</td>\n",
       "      <td>hybrid_gpt_aug</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.098204</td>\n",
       "      <td>0.982036</td>\n",
       "      <td>0.982036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hybrid BM25→FAISS + GPT-aug</td>\n",
       "      <td>hybrid_gpt_aug</td>\n",
       "      <td>hard</td>\n",
       "      <td>0.098193</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.981928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hybrid BM25→FAISS + GPT-aug</td>\n",
       "      <td>hybrid_gpt_aug</td>\n",
       "      <td>all</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hybrid BM25→FAISS + GPT-aug</td>\n",
       "      <td>hybrid_gpt_aug</td>\n",
       "      <td>all</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     test_name         backend   level  Precision@10  \\\n",
       "0  Hybrid BM25→FAISS + GPT-aug  hybrid_gpt_aug    easy      0.099401   \n",
       "1  Hybrid BM25→FAISS + GPT-aug  hybrid_gpt_aug  medium      0.098204   \n",
       "2  Hybrid BM25→FAISS + GPT-aug  hybrid_gpt_aug    hard      0.098193   \n",
       "3  Hybrid BM25→FAISS + GPT-aug  hybrid_gpt_aug     all      0.098600   \n",
       "4  Hybrid BM25→FAISS + GPT-aug  hybrid_gpt_aug     all      0.098600   \n",
       "\n",
       "   Recall@10       MRR  \n",
       "0   0.994012  0.994012  \n",
       "1   0.982036  0.982036  \n",
       "2   0.981928  0.981928  \n",
       "3   0.986000  0.986000  \n",
       "4   0.986000  0.986000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, time, requests\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from huggingface_hub import InferenceClient\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "HF_MODEL       = \"sergeyzh/BERTA\"\n",
    "HF_TOKEN       = os.getenv(\"HF_TOKEN\")     \n",
    "OPENROUTER_KEY = os.getenv(\"OPENROUTER_KEY\") \n",
    "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "CAND_K       = 100   \n",
    "HF_BATCH     = 32        \n",
    "HF_RETRIES   = 3       \n",
    "TOP_K_EVAL   = 10    \n",
    "\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    return re.sub(r\"[^а-яёa-z0-9\\s]+\", \" \", txt.lower()).strip()\n",
    "\n",
    "def gpt_augment(q: str, k: int = 3) -> List[str]:\n",
    "    if not OPENROUTER_KEY:\n",
    "        return [q]\n",
    "    sys_prompt = (\n",
    "        \"Ты ассистент для расширения и исправления запроса пользователя к новостному порталу vesti.ru, чтобы улучшить поиск\"\n",
    "        \"Добавь альтернативные варианты к запросу, поправив орфографию если надо, дополнив смысл, добавляя деталей нужных\"\n",
    "        \"Если запрос явно с опечаткой, поменяй на наиболее вероятную\"\n",
    "        f\"Дай {k} альтернатив через запятую. Без лишних комментариев\"\n",
    "    )\n",
    "    payload = {\n",
    "        \"model\": \"openai/gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": q},\n",
    "        ],\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 64,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"Bearer {OPENROUTER_KEY}\",\n",
    "               \"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        r = requests.post(OPENROUTER_URL, headers=headers,\n",
    "                          json=payload, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        alts = r.json()[\"choices\"][0][\"message\"][\"content\"].split(\",\")\n",
    "        return [q] + [a.strip() for a in alts[:k]]\n",
    "    except Exception as e:\n",
    "        print(\"GPT-augmentation skipped:\", e)\n",
    "        return [q]\n",
    "\n",
    "\n",
    "bm25_tokens = [word_tokenize(clean_text(t)) or [\"dummy\"] for t in tqdm(texts)]\n",
    "bm25        = BM25Okapi(bm25_tokens)\n",
    "\n",
    "hf_client = InferenceClient(model=HF_MODEL, token=HF_TOKEN)\n",
    "\n",
    "def embed_many(\n",
    "    all_texts: List[str],\n",
    "    batch: int = HF_BATCH,\n",
    "    max_retries: int = HF_RETRIES,\n",
    ") -> np.ndarray:\n",
    "    vectors: List[List[float]] = []\n",
    "\n",
    "    for start in tqdm(range(0, len(all_texts), batch),\n",
    "                      desc=\"HF batches\", leave=False):\n",
    "        chunk = all_texts[start:start + batch]\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                feats = hf_client.feature_extraction(chunk)\n",
    "\n",
    "\n",
    "                if isinstance(feats, np.ndarray):\n",
    "                    if feats.size == 0:\n",
    "                        raise ValueError(\"empty ndarray\")\n",
    "                    feats = feats.tolist()\n",
    "\n",
    "                if len(feats) != len(chunk):       \n",
    "                    raise ValueError(\"incomplete batch\")\n",
    "\n",
    "                vectors.extend(feats)\n",
    "                break                              \n",
    "\n",
    "            except Exception as exc:\n",
    "                if attempt == max_retries:\n",
    "                    raise RuntimeError(\n",
    "                        f\"HF embedding failed after {max_retries} retries \"\n",
    "                        f\"for batch starting {start}: {exc}\"\n",
    "                    ) from exc\n",
    "                time.sleep(1.5 * attempt)        \n",
    "\n",
    "    arr = np.asarray(vectors, dtype=np.float32)\n",
    "    arr /= np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12\n",
    "    return arr\n",
    "\n",
    "emb_arr = embed_many(texts)\n",
    "index_faiss = faiss.IndexFlatL2(emb_arr.shape[1])\n",
    "index_faiss.add(emb_arr)\n",
    "\n",
    "id2pos = {doc_id: idx for idx, doc_id in enumerate(ids)}\n",
    "\n",
    "\n",
    "class HybridAugBackend:\n",
    "    \"\"\"BM-25 candidate + cosine FAISS rerank + optional GPT expansion\"\"\"\n",
    "\n",
    "    def _embed_query(self, q: str) -> np.ndarray:\n",
    "        vec = hf_client.feature_extraction([q])[0]\n",
    "        vec = np.asarray(vec, dtype=np.float32)\n",
    "        vec /= np.linalg.norm(vec) + 1e-12\n",
    "        return vec\n",
    "\n",
    "    def search(self, q: str, top_k: int = 10) -> List[Tuple[str, float]]:\n",
    "        variants = gpt_augment(q, k=3)\n",
    "        pool: Dict[str, float] = {}\n",
    "\n",
    "        for v in variants:\n",
    "            scores = bm25.get_scores(word_tokenize(clean_text(v)))\n",
    "            for i in np.argsort(scores)[::-1][:CAND_K]:\n",
    "                did = ids[i]\n",
    "                pool[did] = max(pool.get(did, 0.0), scores[i])\n",
    "\n",
    "        if not pool:\n",
    "            return []\n",
    "\n",
    "        cand_ids = list(pool)\n",
    "        emb_subset = emb_arr[[id2pos[d] for d in cand_ids]]\n",
    "\n",
    "        q_vec = self._embed_query(q)\n",
    "        cos   = emb_subset @ q_vec\n",
    "        best  = cos.argsort()[::-1][:top_k]\n",
    "        return [(cand_ids[i], float(cos[i])) for i in best]\n",
    "\n",
    "\n",
    "backend = HybridAugBackend()\n",
    "\n",
    "df_metrics = run_and_log(\n",
    "    backend      = backend,\n",
    "    q_groups     = q_groups,\n",
    "    backend_name = \"hybrid_gpt_aug\",\n",
    "    test_name    = \"Hybrid BM25→FAISS + GPT-aug\",\n",
    "    top_k        = TOP_K_EVAL,\n",
    "    max_workers  = 20,\n",
    ")\n",
    "\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb464d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
