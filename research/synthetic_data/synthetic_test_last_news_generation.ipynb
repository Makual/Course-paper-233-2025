{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76104/3318184949.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from research.processing.utils import fetch_last_news\n",
    "from multiprocessing import Pool\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd4171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PG_DSN = (\n",
    "    f\"postgresql://{os.getenv('PG_USER')}:\"\n",
    "    f\"{os.getenv('PG_PASS')}@\"\n",
    "    f\"{os.getenv('PG_HOST')}:\"\n",
    "    f\"{os.getenv('PG_PORT')}/\"\n",
    "    f\"{os.getenv('PG_DB')}\"\n",
    ")\n",
    "OPENROUTER_KEY = os.getenv('OPENROUTER_KEY')\n",
    "engine = create_engine(PG_DSN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbb9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_KEYS = {\"easy\", \"medium\", \"hard\"}\n",
    "\n",
    "def parse_gpt_output(raw: str, *, article_id: str = \"\") -> Dict[str, str]:\n",
    "    out = {k: \"\" for k in EXPECTED_KEYS}\n",
    "\n",
    "    for line in raw.splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        m = re.match(r\"^\\s*([\\w]+)\\s*[:\\-–—]\\s*(.+)$\", line)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        k, v = m.group(1).lower(), m.group(2).strip()\n",
    "        if k in EXPECTED_KEYS:\n",
    "            out[k] = v\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85bbdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTSynthesizer:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "    def _gpt_query(self, system_prompt: str, prompt: str) -> str:\n",
    "        messages: List[Dict[str, str]] = []\n",
    "        if system_prompt:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Всегда отвечай на русском языке. \" + system_prompt\n",
    "            })\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"qwen/qwen-turbo\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 512,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    def generate_for_article(self, title: str, anons: str, body: str, *, article_id: str = \"\") -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Для переданной новости (title, anons, body) формирует 3 поисковых запроса\n",
    "        с разными уровнями сложности: лёгкий, средний, сложный.\n",
    "        Возвращает словарь {\"easy\": ..., \"medium\": ..., \"hard\": ...}.\n",
    "        \"\"\"\n",
    "        snippet = body[:200].replace(\"\\n\", \" \")\n",
    "        full_context = f\"Заголовок: {title}\\nАнонс: {anons or ''}\\nТело (фрагмент): {snippet}...\"\n",
    "        system_prompt = (\n",
    "            \"Ты генерируешь три реалистичных поисковых запроса человека в поисковике разного уровня сложности, \"\n",
    "            \"чтобы найти именно эту новость на новостном портале. \"\n",
    "            \"Первый запрос — максимально буквальный, в точности, как в новости\"\n",
    "            \"Второй — чуть более обобщённый (с синонимами). Как бы мог сформулировать эту новость человек\"\n",
    "            \"Третий — максимально абстрактный, где используются контекстные формулировки и косвенные признаки, \"\n",
    "            \"возможны реалестичные опечатки, чтобы тестировать поисковую систему по новостям. Главное, чтобы запросы были, как от реального человека\"\n",
    "        )\n",
    "        prompt = (\n",
    "            f\"{full_context}\\n\\n\"\n",
    "            \"Сгенерируй три отдельных строки запроса человека к новостному порталу, помеченные как:\\n\"\n",
    "            \"easy: \\n\"\n",
    "            \"medium: \\n\"\n",
    "            \"hard: \"\n",
    "        )\n",
    "\n",
    "        raw = self._gpt_query(system_prompt, prompt)\n",
    "\n",
    "        parsed = parse_gpt_output(raw, article_id=article_id)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = fetch_last_news(limit=10000)\n",
    "gpt = GPTSynthesizer(api_key=OPENROUTER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52545061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_row_tuple(args):\n",
    "    art_id, title, anons, body = args\n",
    "    try:\n",
    "        gens = gpt.generate_for_article(title, anons, body, article_id=art_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] article_id={art_id} → {type(e).__name__}: {e}\")\n",
    "        gens = {\"easy\": \"\", \"medium\": \"\", \"hard\": \"\"}\n",
    "    return art_id, gens\n",
    "\n",
    "\n",
    "def generate_synthetic_mp(df, num_workers=None, chunksize=10):\n",
    "    arg_list = [\n",
    "        (row[\"id\"], row[\"title\"], row[\"anons\"], row[\"body\"])\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    synthetic = []\n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        for art_id, gens in tqdm(\n",
    "            pool.imap_unordered(_process_row_tuple, arg_list, chunksize=chunksize),\n",
    "            total=len(arg_list),\n",
    "            desc=\"Generating synthetic queries\",\n",
    "        ):\n",
    "            synthetic.append((art_id, \"easy\",   gens.get(\"easy\", \"\")))\n",
    "            synthetic.append((art_id, \"medium\", gens.get(\"medium\", \"\")))\n",
    "            synthetic.append((art_id, \"hard\",   gens.get(\"hard\", \"\")))\n",
    "\n",
    "    return synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2808ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating synthetic queries: 100%|██████████| 10000/10000 [1:12:12<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "synthetic_queries = generate_synthetic_mp(df_sample, num_workers=None, chunksize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "076f406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 246646.39it/s]\n"
     ]
    }
   ],
   "source": [
    "df_synth = pd.DataFrame(synthetic_queries, columns=[\"article_id\", \"difficulty\", \"query\"])\n",
    "queries_gt: Dict[str, List[str]] = {}\n",
    "for art_id, difficulty, q in tqdm(synthetic_queries):\n",
    "    queries_gt[q] = [art_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de85308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synth.to_csv(\"synthetic_queries_first_10000.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
